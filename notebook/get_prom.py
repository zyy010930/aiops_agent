import ast
import json
import os
import sys
import time
from datetime import datetime, timedelta

import numpy as np
from aliyun.log import LogClient, GetLogsRequest
from matplotlib import pyplot as plt


sys.path.append('..')
# SLS configuration
PROJECT_NAME = "proj-xtrace-a46b97cfdc1332238f714864c014a1b-cn-qingdao"
LOGSTORE_NAME = "logstore-tracing"
REGION = "cn-qingdao"

# Environment variables
STS_ROLE_ARN = os.getenv('ALIBABA_CLOUD_ROLE_ARN', 'acs:ram::1672753017899339:role/tianchi-user-a')
STS_SESSION_NAME = os.getenv('ALIBABA_CLOUD_ROLE_SESSION_NAME', 'my-sls-access')

# CMS æŒ‡æ ‡é…ç½®
CMS_WORKSPACE = "tianchi-workspace"
CMS_ENDPOINT = os.getenv("CMS_ENDPOINT", "cms.cn-qingdao.aliyuncs.com")
try:
    from test_cms_query import TestCMSQuery

    print("âœ… TestCMSQuery imported successfully")
except ImportError as e:
    print(f"âš ï¸ Warning: Could not import TestCMSQuery: {e}")
    print("ðŸ’¡ Please install required dependencies: pip install -r requirements.txt")
    TestCMSQuery = None
# åˆå§‹åŒ–CMSæµ‹è¯•å®¢æˆ·ç«¯ï¼Œç”¨äºŽæŒ‡æ ‡æŸ¥è¯¢
# å¦‚æžœå­˜åœ¨å¯¼å…¥é—®é¢˜é€šè¿‡ç›´æŽ¥åˆ›å»ºç±»ä¿®å¤(except)
try:
    if TestCMSQuery is not None:
        cms_tester = TestCMSQuery()
        cms_tester.setUp()
        print(f"âœ… å·²é€šè¿‡å¯¼å…¥çš„ TestCMSQuery åˆå§‹åŒ–CMSå®¢æˆ·ç«¯")
    else:
        raise ImportError("TestCMSQuery is None")
except:
    print("âš ï¸  TestCMSQuery import failed, creating CMS client directly...")

    import os
    from alibabacloud_cms20240330.client import Client as Cms20240330Client
    from alibabacloud_tea_openapi import models as open_api_models
    from alibabacloud_cms20240330 import models as cms_20240330_models
    from alibabacloud_tea_util import models as util_models


    class DirectCMSClient:
        def __init__(self):
            self.access_key_id = os.getenv('ALIBABA_CLOUD_ACCESS_KEY_ID')
            self.access_key_secret = os.getenv('ALIBABA_CLOUD_ACCESS_KEY_SECRET')
            self.workspace = CMS_WORKSPACE
            self.endpoint = CMS_ENDPOINT

            if not self.access_key_id or not self.access_key_secret:
                raise ValueError("è¯·è®¾ç½®çŽ¯å¢ƒå˜é‡ ALIBABA_CLOUD_ACCESS_KEY_ID å’Œ ALIBABA_CLOUD_ACCESS_KEY_SECRET")

            config = open_api_models.Config(
                access_key_id=self.access_key_id,
                access_key_secret=self.access_key_secret,
            )
            config.endpoint = self.endpoint
            self.cms_client = Cms20240330Client(config)

        def _execute_spl_query(self, query: str, from_time: int = None, to_time: int = None):
            """æ‰§è¡ŒSPLæŸ¥è¯¢"""
            if from_time is None:
                from_time = int(time.time()) - 60 * 60 * 1
            if to_time is None:
                to_time = int(time.time())

            try:
                headers = cms_20240330_models.GetEntityStoreDataHeaders()
                request = cms_20240330_models.GetEntityStoreDataRequest(
                    query=query,
                    from_=from_time,
                    to=to_time
                )
                runtime = util_models.RuntimeOptions()
                response = self.cms_client.get_entity_store_data_with_options(
                    self.workspace, request, headers, runtime
                )
                return response.body
            except Exception as e:
                print(f"âŒ CMSæŸ¥è¯¢é”™è¯¯: {e}")
                return None


    cms_tester = DirectCMSClient()
    print(f"âœ… CMS client created directly")

print(f"ðŸ”§ CMSå®¢æˆ·ç«¯å·²åˆå§‹åŒ–")
print(f"ðŸ”§ workspace: {CMS_WORKSPACE}")
print(f"ðŸ”§ Endpoint: {CMS_ENDPOINT}")


def get_sts_credentials():
    try:
        from aliyunsdkcore.client import AcsClient
        from aliyunsdksts.request.v20150401 import AssumeRoleRequest

        MAIN_ACCOUNT_ACCESS_KEY_ID = os.getenv('ALIBABA_CLOUD_ACCESS_KEY_ID')
        MAIN_ACCOUNT_ACCESS_KEY_SECRET = os.getenv('ALIBABA_CLOUD_ACCESS_KEY_SECRET')
        ALIBABA_CLOUD_ROLE_ARN = os.getenv('ALIBABA_CLOUD_ROLE_ARN', 'acs:ram::1672753017899339:role/tianchi-user-a')
        STS_SESSION_NAME = os.getenv('ALIBABA_CLOUD_ROLE_SESSION_NAME', 'my-sls-access')

        if not MAIN_ACCOUNT_ACCESS_KEY_ID or not MAIN_ACCOUNT_ACCESS_KEY_SECRET:
            return None, None, None

        client = AcsClient(MAIN_ACCOUNT_ACCESS_KEY_ID, MAIN_ACCOUNT_ACCESS_KEY_SECRET, REGION)
        request = AssumeRoleRequest.AssumeRoleRequest()
        request.set_RoleArn(ALIBABA_CLOUD_ROLE_ARN)
        request.set_RoleSessionName(STS_SESSION_NAME)
        request.set_DurationSeconds(3600)

        response = client.do_action_with_exception(request)
        response_data = json.loads(response)
        credentials = response_data['Credentials']
        return (credentials['AccessKeyId'], credentials['AccessKeySecret'], credentials['SecurityToken'])
    except Exception as e:
        print(f"âŒ èŽ·å–STSå‡­è¯å¤±è´¥: {e}")
        return None, None, None


temp_access_key_id, temp_access_key_secret, security_token = get_sts_credentials()
if not temp_access_key_id:
    print("âŒ æ— æ³•èŽ·å–STSä¸´æ—¶å‡­è¯ï¼Œåˆ†æžç»ˆæ­¢")

try:
    from aliyun.log import LogClient

    sls_endpoint = os.getenv("SLS_ENDPOINT", "cn-qingdao.log.aliyuncs.com")
    log_client = LogClient(sls_endpoint, temp_access_key_id, temp_access_key_secret, security_token)
except Exception as e:
    print(f"âŒ åˆ›å»ºSLSå®¢æˆ·ç«¯å¤±è´¥: {e}")


def read_input_data(input_file_path):
    """
    Read and parse input data from JSONL file

    Args:
        input_file_path: Path to the input JSONL file

    Returns:
        list: List of parsed JSON objects
    """
    data = []

    try:
        with open(input_file_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line:  # Skip empty lines
                    try:
                        item = json.loads(line)
                        data.append(item)
                    except json.JSONDecodeError as e:
                        print(f"âš ï¸ Failed to parse line: {line[:100]}... Error: {e}")
                        continue

        print(f"âœ… Successfully read {len(data)} records from {input_file_path}")
        return data

    except FileNotFoundError:
        print(f"âŒ Input file not found: {input_file_path}")
        return []
    except Exception as e:
        print(f"âŒ Failed to read input file: {e}")
        return []


def detect_anomaly(normal_values, pre_values, post_values, threshold=1.5):
    """
    æ£€æµ‹æ­£å¸¸æ—¶æ®µçš„æŒ‡æ ‡æ˜¯å¦æ˜Žæ˜¾é«˜äºŽå‰åŽæ—¶æ®µ

    Args:
        normal_values: æ­£å¸¸æ—¶æ®µçš„æŒ‡æ ‡å€¼åˆ—è¡¨
        pre_values: å‰10åˆ†é’Ÿçš„æŒ‡æ ‡å€¼åˆ—è¡¨
        post_values: åŽ10åˆ†é’Ÿçš„æŒ‡æ ‡å€¼åˆ—è¡¨
        threshold: å¼‚å¸¸é˜ˆå€¼ï¼Œæ­£å¸¸æ—¶æ®µå¹³å‡å€¼è¶…è¿‡å‰åŽæ—¶æ®µå¹³å‡å€¼çš„å€æ•°

    Returns:
        tuple: (æ˜¯å¦å¼‚å¸¸, æ­£å¸¸æ—¶æ®µå¹³å‡å€¼, å‰æ—¶æ®µå¹³å‡å€¼, åŽæ—¶æ®µå¹³å‡å€¼)
    """
    if not normal_values or not pre_values or not post_values:
        print("âš ï¸ ç¼ºå°‘æ•°æ®ï¼Œæ— æ³•è¿›è¡Œå¼‚å¸¸æ£€æµ‹")
        return False, 0, 0, 0

    # è®¡ç®—å„æ—¶æ®µå¹³å‡å€¼
    normal_avg = np.mean(normal_values)
    pre_avg = np.mean(pre_values)
    post_avg = np.mean(post_values)

    # è®¡ç®—å‰åŽæ—¶æ®µçš„å¹³å‡æ°´å¹³
    baseline_avg = np.mean([pre_avg, post_avg])

    # åˆ¤æ–­æ˜¯å¦å¼‚å¸¸ï¼šæ­£å¸¸æ—¶æ®µå¹³å‡å€¼æ˜Žæ˜¾é«˜äºŽåŸºçº¿
    is_anomaly = (normal_avg > baseline_avg * threshold or normal_avg > baseline_avg + 20) and pre_avg < normal_avg and post_avg < normal_avg

    return is_anomaly, normal_avg, pre_avg, post_avg


def split_time_period_data(timestamps, values, pre10_end, normal_end):
    """
    å°†æ•°æ®æŒ‰æ—¶é—´åˆ†å‰²ä¸ºå‰10åˆ†é’Ÿã€æ­£å¸¸æ—¶æ®µã€åŽ10åˆ†é’Ÿ

    Args:
        timestamps: æ—¶é—´æˆ³åˆ—è¡¨
        values: æŒ‡æ ‡å€¼åˆ—è¡¨
        pre10_end: å‰10åˆ†é’Ÿç»“æŸæ—¶é—´
        normal_end: æ­£å¸¸æ—¶æ®µç»“æŸæ—¶é—´

    Returns:
        tuple: (å‰10åˆ†é’Ÿå€¼åˆ—è¡¨, æ­£å¸¸æ—¶æ®µå€¼åˆ—è¡¨, åŽ10åˆ†é’Ÿå€¼åˆ—è¡¨)
    """
    pre_values = []
    normal_values = []
    post_values = []

    for ts, val in zip(timestamps, values):
        if ts <= pre10_end:
            pre_values.append(val)
        elif ts <= normal_end:
            normal_values.append(val)
        else:
            post_values.append(val)

    return pre_values, normal_values, post_values

def get_result(result):
    # 1. ä»Žresultä¸­æå–dataåˆ—è¡¨ï¼ˆåŽŸå§‹ç»“æžœæ˜¯å­—å…¸ï¼Œç›´æŽ¥ç”¨é”®è®¿é—®ï¼‰
    # æ³¨æ„ï¼šæ ¹æ®ä½ çš„æ‰“å°ç»“æžœï¼Œresultæ˜¯å­—å…¸ï¼Œä¸æ˜¯å¯¹è±¡ï¼Œæ‰€ä»¥ç”¨['data']è€Œéž.result.data
    data_list = result.data
    if not data_list:
        print(f"âš ï¸ ç»“æžœä¸­ 'data' å­—æ®µä¸ºç©º")
        return [], []

    # 2. æå–æ—¶é—´æˆ³åˆ—è¡¨ï¼ˆ__ts__åˆ—ï¼Œå¯¹åº”data_list[0][2]ï¼‰å’ŒCPUå€¼åˆ—è¡¨ï¼ˆ__value__åˆ—ï¼Œå¯¹åº”data_list[0][3]ï¼‰
    # data_list[0]æ˜¯ç¬¬ä¸€è¡Œæ•°æ®ï¼Œ[2]æ˜¯ç¬¬ä¸‰åˆ—ï¼ˆ__ts__ï¼‰ï¼Œ[3]æ˜¯ç¬¬å››åˆ—ï¼ˆ__value__ï¼‰
    ts_str = data_list[0][2]  # æ ¼å¼ï¼š"[1758037443000000000, 1758037503000000000, ...]"
    cpu_str = data_list[0][3]  # æ ¼å¼ï¼š"[0.00231595..., 0.01094574..., ...]"

    # 3. å°†å­—ç¬¦ä¸²åˆ—è¡¨è½¬ä¸ºPythonåˆ—è¡¨ï¼ˆç”¨ast.literal_evalï¼Œé¿å…evalçš„å®‰å…¨é£Žé™©ï¼‰
    ts_list = ast.literal_eval(ts_str)  # æ—¶é—´æˆ³åˆ—è¡¨ï¼ˆå•ä½ï¼šçº³ç§’ï¼‰
    cpu_list = ast.literal_eval(cpu_str)  # CPUæ•°å€¼åˆ—è¡¨

    # 4. å¤„ç†æ—¶é—´æˆ³ï¼šçº³ç§’è½¬ç§’ï¼ˆé™¤ä»¥1e9ï¼‰ï¼Œå†è½¬ä¸ºdatetimeæ ¼å¼
    timestamps = []
    for ts in ts_list:
        # çº³ç§’ â†’ ç§’ï¼ˆé™¤ä»¥10^9ï¼‰ï¼Œå†è½¬ä¸ºdatetime
        dt = datetime.fromtimestamp(ts / 1e9)
        timestamps.append(dt)

    # 5. ç¡®ä¿CPUå€¼ä¸ºfloatç±»åž‹ï¼ˆé˜²æ­¢åŽŸå§‹æ•°æ®æ˜¯å­—ç¬¦ä¸²ï¼‰
    cpu_values = [float(val) for val in cpu_list]

    return timestamps, cpu_values


def analyze_network(normal_start, normal_end, Target_ECS, show):
    # 1. è®¡ç®—ä¸‰ä¸ªæ—¶æ®µçš„æ—¶é—´æˆ³ï¼ˆè½¬ä¸ºintç±»åž‹ï¼ŒCMSæŸ¥è¯¢è¦æ±‚ï¼‰
    # å‰10åˆ†é’Ÿï¼šnormal_start - 10min åˆ° normal_start
    pre10_start = int((normal_start - timedelta(minutes=10)).timestamp())
    pre10_end = int(normal_start.timestamp())
    # æ­£å¸¸æ—¶æ®µï¼šnormal_start åˆ° normal_end
    normal_start_ts = int(normal_start.timestamp())
    normal_end_ts = int(normal_end.timestamp())
    # åŽ10åˆ†é’Ÿï¼šnormal_end åˆ° normal_end + 10min
    post10_start = int(normal_end.timestamp())
    post10_end = int((normal_end + timedelta(minutes=10)).timestamp())

    # 2. æŸ¥è¯¢è¯­å¥
    query_list = []
    query_list.append(f"""
    .metricstore with(project='workspace-tianchi-2025-0828-01', metricstore='aliyun-prom-rw-16e081404ce19b5294c7967ff61d')
    | prom-call promql_query_range('sum(irate(node_netstat_Tcp_OutRsts{{instanceId=~"{Target_ECS}"}}[1m]))', '60s')
    """)

    query_list.append(f"""
    .metricstore with(project='workspace-tianchi-2025-0828-01', metricstore='aliyun-prom-rw-16e081404ce19b5294c7967ff61d')
    | prom-call promql_query_range('sum(irate(node_netstat_TcpExt_TCPSynRetrans{{instanceId=~"{Target_ECS}"}}[1m]))', '60s')
    """)

    query_list.append(f"""
    .metricstore with(project='workspace-tianchi-2025-0828-01', metricstore='aliyun-prom-rw-16e081404ce19b5294c7967ff61d')
    | prom-call promql_query_range('sum(irate(node_netstat_Tcp_RetransSegs{{instanceId=~"{Target_ECS}"}}[1m]))', '60s')
    """)

    query_list.append(f"""
    .metricstore with(project='workspace-tianchi-2025-0828-01', metricstore='aliyun-prom-rw-16e081404ce19b5294c7967ff61d')
    | prom-call promql_query_range('sum(irate(node_netstat_Tcp_InErrs{{instanceId=~"{Target_ECS}"}}[1m]))', '60s')
    """)

    anomalyNum = 0
    for query in query_list:
        result = cms_tester._execute_spl_query(
            query.strip(),
            from_time=pre10_start,
            to_time=post10_end
        )

        timestamps, network = get_result(result)
        if len(timestamps) == 0:
            continue

        # 4. åˆ†å‰²ä¸‰ä¸ªæ—¶æ®µçš„æ•°æ®
        pre10_end_dt = datetime.fromtimestamp(pre10_end)
        normal_end_dt = datetime.fromtimestamp(normal_end_ts)
        pre_values, normal_values, post_values = split_time_period_data(
            timestamps, network, pre10_end_dt, normal_end_dt
        )

        # 5. å¼‚å¸¸æ£€æµ‹
        is_anomaly, normal_avg, pre_avg, post_avg = detect_anomaly(
            normal_values, pre_values, post_values
        )
        max_cpu = max(normal_values)

        # 6. è¾“å‡ºå¼‚å¸¸æ£€æµ‹ç»“æžœ
        print(f"\ncpuå¼‚å¸¸æ£€æµ‹ç»“æžœ:")
        print(f"å‰10åˆ†é’Ÿå¹³å‡å€¼: {pre_avg:.4f}")
        print(f"æ£€æµ‹æ—¶æ®µå¹³å‡å€¼: {normal_avg:.4f}")
        print(f"åŽ10åˆ†é’Ÿå¹³å‡å€¼: {post_avg:.4f}")
        print(f"æœ€å¤§CPUä½¿ç”¨çŽ‡: {max_cpu:.4f}")

        if is_anomaly:
            print(f"ðŸ”´ å¼‚å¸¸æ£€æµ‹: æ£€æµ‹æ—¶æ®µnetworkæ˜Žæ˜¾é«˜äºŽå‰åŽæ—¶æ®µ!")
            anomalyNum += 1
        else:
            print(f"ðŸŸ¢ å¼‚å¸¸æ£€æµ‹: æ£€æµ‹æ—¶æ®µnetworkå¤„äºŽæ­£å¸¸èŒƒå›´")

        if show:
            plt.figure(figsize=(12, 6))

            plt.plot(timestamps, network, marker='o', linestyle='-', color='b')
            pre10_start_dt = datetime.fromtimestamp(pre10_start)  # å‰10åˆ†é’Ÿå¼€å§‹ï¼ˆdatetimeï¼‰
            pre10_end_dt = datetime.fromtimestamp(pre10_end)  # å‰10åˆ†é’Ÿç»“æŸï¼ˆdatetimeï¼‰
            normal_start_dt = datetime.fromtimestamp(normal_start_ts)  # ç›®æ ‡æ—¶æ®µå¼€å§‹ï¼ˆdatetimeï¼‰
            normal_end_dt = datetime.fromtimestamp(normal_end_ts)  # ç›®æ ‡æ—¶æ®µç»“æŸï¼ˆdatetimeï¼‰
            post10_start_dt = datetime.fromtimestamp(post10_start)  # åŽ10åˆ†é’Ÿå¼€å§‹ï¼ˆdatetimeï¼‰
            post10_end_dt = datetime.fromtimestamp(post10_end)  # åŽ10åˆ†é’Ÿç»“æŸï¼ˆdatetimeï¼‰
            # ç”¨é˜´å½±æ ‡è®°ä¸‰ä¸ªæ—¶æ®µ
            plt.axvspan(pre10_start_dt, pre10_end_dt, color='lightgreen', alpha=0.3, label='å‰10åˆ†é’Ÿ')
            plt.axvspan(normal_start_dt, normal_end_dt, color='lightcoral', alpha=0.3, label='ç›®æ ‡æ—¶æ®µ')
            plt.axvspan(post10_start_dt, post10_end_dt, color='lightblue', alpha=0.3, label='åŽ10åˆ†é’Ÿ')

            plt.show()
    return anomalyNum

def analyze_gc(normal_start, normal_end, Target_ECS, show):
    # 1. è®¡ç®—ä¸‰ä¸ªæ—¶æ®µçš„æ—¶é—´æˆ³ï¼ˆè½¬ä¸ºintç±»åž‹ï¼ŒCMSæŸ¥è¯¢è¦æ±‚ï¼‰
    # å‰10åˆ†é’Ÿï¼šnormal_start - 10min åˆ° normal_start
    pre10_start = int((normal_start - timedelta(minutes=10)).timestamp())
    pre10_end = int(normal_start.timestamp())
    # æ­£å¸¸æ—¶æ®µï¼šnormal_start åˆ° normal_end
    normal_start_ts = int(normal_start.timestamp())
    normal_end_ts = int(normal_end.timestamp())
    # åŽ10åˆ†é’Ÿï¼šnormal_end åˆ° normal_end + 10min
    post10_start = int(normal_end.timestamp())
    post10_end = int((normal_end + timedelta(minutes=10)).timestamp())

    # 2. æŸ¥è¯¢è¯­å¥
    # query= f"""
    # .metricstore with(project='workspace-tianchi-2025-0828-01', metricstore='aliyun-prom-arms-67b1a0473064fa06ae361d42ad')
    # | prom-call promql_query_range('sum (sum_over_time_lorc(arms_jvm_gc_delta{{acs_arms_service_id="hwx28v3j7p@680213ea70b15a61c56ed",gen="old",host=~".*", }}[1m]))', '60s')
    # """
    query = f"""
    .entity_set with(domain='apm', name='apm.service', query=`service='inventory'`)
    | entity-call get_metric('apm', 'apm.metric.jvm', 'arms_jvm_gc_delta', 'range', '1m')
    """

    anomalyNum = 0
    result = cms_tester._execute_spl_query(
        query.strip(),
        from_time=pre10_start,
        to_time=post10_end
    )
    print(result)

    timestamps, network = get_result(result)

    # 4. åˆ†å‰²ä¸‰ä¸ªæ—¶æ®µçš„æ•°æ®
    pre10_end_dt = datetime.fromtimestamp(pre10_end)
    normal_end_dt = datetime.fromtimestamp(normal_end_ts)
    pre_values, normal_values, post_values = split_time_period_data(
        timestamps, network, pre10_end_dt, normal_end_dt
    )

    # 5. å¼‚å¸¸æ£€æµ‹
    is_anomaly, normal_avg, pre_avg, post_avg = detect_anomaly(
        normal_values, pre_values, post_values
    )
    max_gc = max(normal_values)

    # 6. è¾“å‡ºå¼‚å¸¸æ£€æµ‹ç»“æžœ
    print(f"\ncpuå¼‚å¸¸æ£€æµ‹ç»“æžœ:")
    print(f"å‰10åˆ†é’Ÿå¹³å‡å€¼: {pre_avg:.4f}")
    print(f"æ£€æµ‹æ—¶æ®µå¹³å‡å€¼: {normal_avg:.4f}")
    print(f"åŽ10åˆ†é’Ÿå¹³å‡å€¼: {post_avg:.4f}")
    print(f"æœ€å¤§gcæ¬¡æ•°: {max_gc:.4f}")

    if is_anomaly:
        print(f"ðŸ”´ å¼‚å¸¸æ£€æµ‹: æ£€æµ‹æ—¶æ®µgcæ˜Žæ˜¾é«˜äºŽå‰åŽæ—¶æ®µ!")
        return True
    else:
        print(f"ðŸŸ¢ å¼‚å¸¸æ£€æµ‹: æ£€æµ‹æ—¶æ®µgcå¤„äºŽæ­£å¸¸èŒƒå›´")

    if show:
        plt.figure(figsize=(12, 6))

        plt.plot(timestamps, network, marker='o', linestyle='-', color='b')
        pre10_start_dt = datetime.fromtimestamp(pre10_start)  # å‰10åˆ†é’Ÿå¼€å§‹ï¼ˆdatetimeï¼‰
        pre10_end_dt = datetime.fromtimestamp(pre10_end)  # å‰10åˆ†é’Ÿç»“æŸï¼ˆdatetimeï¼‰
        normal_start_dt = datetime.fromtimestamp(normal_start_ts)  # ç›®æ ‡æ—¶æ®µå¼€å§‹ï¼ˆdatetimeï¼‰
        normal_end_dt = datetime.fromtimestamp(normal_end_ts)  # ç›®æ ‡æ—¶æ®µç»“æŸï¼ˆdatetimeï¼‰
        post10_start_dt = datetime.fromtimestamp(post10_start)  # åŽ10åˆ†é’Ÿå¼€å§‹ï¼ˆdatetimeï¼‰
        post10_end_dt = datetime.fromtimestamp(post10_end)  # åŽ10åˆ†é’Ÿç»“æŸï¼ˆdatetimeï¼‰
        # ç”¨é˜´å½±æ ‡è®°ä¸‰ä¸ªæ—¶æ®µ
        plt.axvspan(pre10_start_dt, pre10_end_dt, color='lightgreen', alpha=0.3, label='å‰10åˆ†é’Ÿ')
        plt.axvspan(normal_start_dt, normal_end_dt, color='lightcoral', alpha=0.3, label='ç›®æ ‡æ—¶æ®µ')
        plt.axvspan(post10_start_dt, post10_end_dt, color='lightblue', alpha=0.3, label='åŽ10åˆ†é’Ÿ')

        plt.show()
    return False

if __name__ == "__main__":
    serveice_list = []
    problem_id = "151"
    input_data = read_input_data("../Bæ¦œé¢˜ç›®.jsonl")
    anomaly_list = []
    for problem_data in input_data:
        problem_id = problem_data.get("problem_id", "unknown")
        problem_id = "055"
        time_range = problem_data.get("time_range", "")
        candidate_root_causes = problem_data.get("candidate_root_causes", [])
        alarm_rules = problem_data.get("alarm_rules", [])
        start_str, end_str = time_range.split(' ~ ')
        normal_start = datetime.strptime(start_str.strip(), "%Y-%m-%d %H:%M:%S")
        normal_end = datetime.strptime(end_str.strip(), "%Y-%m-%d %H:%M:%S")
        # if problem_data.get("alarm_rules")[0] == 'greyFailure':
        if problem_data.get("problem_id") == problem_id:
            anomaly = analyze_gc(normal_start, normal_end, "inventory", False)
            if anomaly:
                anomaly_list.append(problem_id)
            # for candidate in candidate_root_causes:
            #     if '.' in candidate and candidate.endswith('.cpu'):
            #         service = candidate.split('.')[0]
            #         if service[1] != '-':
            #             continue
            #
            #         anomaly = analyze_network(normal_start, normal_end, service, False)
            #         if anomaly >= 2:
            #             anomaly_list.append({problem_id, service})
    print(anomaly_list)

import ast
import json
import os
import sys
import time
from datetime import datetime, timedelta

import numpy as np
from aliyun.log import LogClient, GetLogsRequest
from matplotlib import pyplot as plt

sys.path.append('..')

# SLS configuration
PROJECT_NAME = "proj-xtrace-a46b97cfdc1332238f714864c014a1b-cn-qingdao"
LOGSTORE_NAME = "logstore-tracing"
REGION = "cn-qingdao"

# Environment variables
STS_ROLE_ARN = os.getenv('ALIBABA_CLOUD_ROLE_ARN', 'acs:ram::1672753017899339:role/tianchi-user-a')
STS_SESSION_NAME = os.getenv('ALIBABA_CLOUD_ROLE_SESSION_NAME', 'my-sls-access')

# CMS æŒ‡æ ‡é…ç½®
CMS_WORKSPACE = "tianchi-workspace"
CMS_ENDPOINT = os.getenv("CMS_ENDPOINT", "cms.cn-qingdao.aliyuncs.com")
try:
    from test_cms_query import TestCMSQuery

    print("âœ… TestCMSQuery imported successfully")
except ImportError as e:
    print(f"âš ï¸ Warning: Could not import TestCMSQuery: {e}")
    print("ğŸ’¡ Please install required dependencies: pip install -r requirements.txt")
    TestCMSQuery = None
# åˆå§‹åŒ–CMSæµ‹è¯•å®¢æˆ·ç«¯ï¼Œç”¨äºæŒ‡æ ‡æŸ¥è¯¢
# å¦‚æœå­˜åœ¨å¯¼å…¥é—®é¢˜é€šè¿‡ç›´æ¥åˆ›å»ºç±»ä¿®å¤(except)
try:
    if TestCMSQuery is not None:
        cms_tester = TestCMSQuery()
        cms_tester.setUp()
        print(f"âœ… å·²é€šè¿‡å¯¼å…¥çš„ TestCMSQuery åˆå§‹åŒ–CMSå®¢æˆ·ç«¯")
    else:
        raise ImportError("TestCMSQuery is None")
except:
    print("âš ï¸  TestCMSQuery import failed, creating CMS client directly...")

    import os
    from alibabacloud_cms20240330.client import Client as Cms20240330Client
    from alibabacloud_tea_openapi import models as open_api_models
    from alibabacloud_cms20240330 import models as cms_20240330_models
    from alibabacloud_tea_util import models as util_models


    class DirectCMSClient:
        def __init__(self):
            self.access_key_id = os.getenv('ALIBABA_CLOUD_ACCESS_KEY_ID')
            self.access_key_secret = os.getenv('ALIBABA_CLOUD_ACCESS_KEY_SECRET')
            self.workspace = CMS_WORKSPACE
            self.endpoint = CMS_ENDPOINT

            if not self.access_key_id or not self.access_key_secret:
                raise ValueError("è¯·è®¾ç½®ç¯å¢ƒå˜é‡ ALIBABA_CLOUD_ACCESS_KEY_ID å’Œ ALIBABA_CLOUD_ACCESS_KEY_SECRET")

            config = open_api_models.Config(
                access_key_id=self.access_key_id,
                access_key_secret=self.access_key_secret,
            )
            config.endpoint = self.endpoint
            self.cms_client = Cms20240330Client(config)

        def _execute_spl_query(self, query: str, from_time: int = None, to_time: int = None):
            """æ‰§è¡ŒSPLæŸ¥è¯¢"""
            if from_time is None:
                from_time = int(time.time()) - 60 * 60 * 1
            if to_time is None:
                to_time = int(time.time())

            try:
                headers = cms_20240330_models.GetEntityStoreDataHeaders()
                request = cms_20240330_models.GetEntityStoreDataRequest(
                    query=query,
                    from_=from_time,
                    to=to_time
                )
                runtime = util_models.RuntimeOptions()
                response = self.cms_client.get_entity_store_data_with_options(
                    self.workspace, request, headers, runtime
                )
                return response.body
            except Exception as e:
                print(f"âŒ CMSæŸ¥è¯¢é”™è¯¯: {e}")
                return None


    cms_tester = DirectCMSClient()
    print(f"âœ… CMS client created directly")

print(f"ğŸ”§ CMSå®¢æˆ·ç«¯å·²åˆå§‹åŒ–")
print(f"ğŸ”§ workspace: {CMS_WORKSPACE}")
print(f"ğŸ”§ Endpoint: {CMS_ENDPOINT}")


def get_sts_credentials():
    try:
        from aliyunsdkcore.client import AcsClient
        from aliyunsdksts.request.v20150401 import AssumeRoleRequest

        MAIN_ACCOUNT_ACCESS_KEY_ID = os.getenv('ALIBABA_CLOUD_ACCESS_KEY_ID')
        MAIN_ACCOUNT_ACCESS_KEY_SECRET = os.getenv('ALIBABA_CLOUD_ACCESS_KEY_SECRET')
        ALIBABA_CLOUD_ROLE_ARN = os.getenv('ALIBABA_CLOUD_ROLE_ARN', 'acs:ram::1672753017899339:role/tianchi-user-a')
        STS_SESSION_NAME = os.getenv('ALIBABA_CLOUD_ROLE_SESSION_NAME', 'my-sls-access')

        if not MAIN_ACCOUNT_ACCESS_KEY_ID or not MAIN_ACCOUNT_ACCESS_KEY_SECRET:
            return None, None, None

        client = AcsClient(MAIN_ACCOUNT_ACCESS_KEY_ID, MAIN_ACCOUNT_ACCESS_KEY_SECRET, REGION)
        request = AssumeRoleRequest.AssumeRoleRequest()
        request.set_RoleArn(ALIBABA_CLOUD_ROLE_ARN)
        request.set_RoleSessionName(STS_SESSION_NAME)
        request.set_DurationSeconds(3600)

        response = client.do_action_with_exception(request)
        response_data = json.loads(response)
        credentials = response_data['Credentials']
        return (credentials['AccessKeyId'], credentials['AccessKeySecret'], credentials['SecurityToken'])
    except Exception as e:
        print(f"âŒ è·å–STSå‡­è¯å¤±è´¥: {e}")
        return None, None, None


temp_access_key_id, temp_access_key_secret, security_token = get_sts_credentials()
if not temp_access_key_id:
    print("âŒ æ— æ³•è·å–STSä¸´æ—¶å‡­è¯ï¼Œåˆ†æç»ˆæ­¢")

try:
    from aliyun.log import LogClient

    sls_endpoint = os.getenv("SLS_ENDPOINT", "cn-qingdao.log.aliyuncs.com")
    log_client = LogClient(sls_endpoint, temp_access_key_id, temp_access_key_secret, security_token)
except Exception as e:
    print(f"âŒ åˆ›å»ºSLSå®¢æˆ·ç«¯å¤±è´¥: {e}")


def read_input_data(input_file_path):
    """
    Read and parse input data from JSONL file

    Args:
        input_file_path: Path to the input JSONL file

    Returns:
        list: List of parsed JSON objects
    """
    data = []

    try:
        with open(input_file_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line:  # Skip empty lines
                    try:
                        item = json.loads(line)
                        data.append(item)
                    except json.JSONDecodeError as e:
                        print(f"âš ï¸ Failed to parse line: {line[:100]}... Error: {e}")
                        continue

        print(f"âœ… Successfully read {len(data)} records from {input_file_path}")
        return data

    except FileNotFoundError:
        print(f"âŒ Input file not found: {input_file_path}")
        return []
    except Exception as e:
        print(f"âŒ Failed to read input file: {e}")
        return []


def detect_anomaly(normal_values, pre_values, post_values, threshold=1.5):
    """
    æ£€æµ‹æ­£å¸¸æ—¶æ®µçš„æŒ‡æ ‡æ˜¯å¦æ˜æ˜¾é«˜äºå‰åæ—¶æ®µ

    Args:
        normal_values: æ­£å¸¸æ—¶æ®µçš„æŒ‡æ ‡å€¼åˆ—è¡¨
        pre_values: å‰10åˆ†é’Ÿçš„æŒ‡æ ‡å€¼åˆ—è¡¨
        post_values: å10åˆ†é’Ÿçš„æŒ‡æ ‡å€¼åˆ—è¡¨
        threshold: å¼‚å¸¸é˜ˆå€¼ï¼Œæ­£å¸¸æ—¶æ®µå¹³å‡å€¼è¶…è¿‡å‰åæ—¶æ®µå¹³å‡å€¼çš„å€æ•°

    Returns:
        tuple: (æ˜¯å¦å¼‚å¸¸, æ­£å¸¸æ—¶æ®µå¹³å‡å€¼, å‰æ—¶æ®µå¹³å‡å€¼, åæ—¶æ®µå¹³å‡å€¼)
    """
    if not normal_values or not pre_values or not post_values:
        print("âš ï¸ ç¼ºå°‘æ•°æ®ï¼Œæ— æ³•è¿›è¡Œå¼‚å¸¸æ£€æµ‹")
        return False, 0, 0, 0

    # è®¡ç®—å„æ—¶æ®µå¹³å‡å€¼
    normal_avg = np.mean(normal_values)
    pre_avg = np.mean(pre_values)
    post_avg = np.mean(post_values)

    # è®¡ç®—å‰åæ—¶æ®µçš„å¹³å‡æ°´å¹³
    baseline_avg = np.mean([pre_avg, post_avg])

    # å¦‚æœåŸºçº¿å¹³å‡å€¼å¤§äº40ï¼Œåˆ™å°†é˜ˆå€¼è°ƒä½
    if baseline_avg > 40:
        threshold = 1.25

    # åˆ¤æ–­æ˜¯å¦å¼‚å¸¸ï¼šæ­£å¸¸æ—¶æ®µå¹³å‡å€¼æ˜æ˜¾é«˜äºåŸºçº¿
    is_anomaly = normal_avg > baseline_avg * threshold and pre_avg < normal_avg and post_avg < normal_avg

    return is_anomaly, normal_avg, pre_avg, post_avg


def split_time_period_data(timestamps, values, pre10_end, normal_end):
    """
    å°†æ•°æ®æŒ‰æ—¶é—´åˆ†å‰²ä¸ºå‰10åˆ†é’Ÿã€æ­£å¸¸æ—¶æ®µã€å10åˆ†é’Ÿ

    Args:
        timestamps: æ—¶é—´æˆ³åˆ—è¡¨
        values: æŒ‡æ ‡å€¼åˆ—è¡¨
        pre10_end: å‰10åˆ†é’Ÿç»“æŸæ—¶é—´
        normal_end: æ­£å¸¸æ—¶æ®µç»“æŸæ—¶é—´

    Returns:
        tuple: (å‰10åˆ†é’Ÿå€¼åˆ—è¡¨, æ­£å¸¸æ—¶æ®µå€¼åˆ—è¡¨, å10åˆ†é’Ÿå€¼åˆ—è¡¨)
    """
    pre_values = []
    normal_values = []
    post_values = []

    for ts, val in zip(timestamps, values):
        if ts <= pre10_end:
            pre_values.append(val)
        elif ts <= normal_end:
            normal_values.append(val)
        else:
            post_values.append(val)

    return pre_values, normal_values, post_values


def get_info(start_time, end_time, Target_service):
    # ä½¿ç”¨å¼‚å¸¸æ£€æµ‹è¿›è¡ŒæŸ¥è¯¢aggregate_node_cpu_usage
    query = f"""
    .entity_set with(domain='k8s', name='k8s.pod', query=`pod_ip = ''10.53.56.8'' and pod=''fraud-detection-5df5cbfd4c-vbmh6'' and namespace = ''cms-demo''`)
    | entity-call get_metric('k8s', 'k8s.metric.high_level_metric_pod', 'pod_network_receive_rate', 'range', '1m')
    """

    print(f"ğŸ” Query: {query.strip()}")

    try:
        cpu = 0.0
        result = cms_tester._execute_spl_query(
            query.strip(),
            from_time=start_time,
            to_time=end_time
        )
        print(result)
    except Exception as e:
        print(f"âŒ å¼‚å¸¸æ£€æµ‹è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        return False


def get_result(result):
    # 1. ä»resultä¸­æå–dataåˆ—è¡¨ï¼ˆåŸå§‹ç»“æœæ˜¯å­—å…¸ï¼Œç›´æ¥ç”¨é”®è®¿é—®ï¼‰
    # æ³¨æ„ï¼šæ ¹æ®ä½ çš„æ‰“å°ç»“æœï¼Œresultæ˜¯å­—å…¸ï¼Œä¸æ˜¯å¯¹è±¡ï¼Œæ‰€ä»¥ç”¨['data']è€Œé.result.data
    data_list = result.data
    if not data_list:
        print(f"âš ï¸ ç»“æœä¸­ 'data' å­—æ®µä¸ºç©º")
        return [], []

    # 2. æå–æ—¶é—´æˆ³åˆ—è¡¨ï¼ˆ__ts__åˆ—ï¼Œå¯¹åº”data_list[0][2]ï¼‰å’ŒCPUå€¼åˆ—è¡¨ï¼ˆ__value__åˆ—ï¼Œå¯¹åº”data_list[0][3]ï¼‰
    # data_list[0]æ˜¯ç¬¬ä¸€è¡Œæ•°æ®ï¼Œ[2]æ˜¯ç¬¬ä¸‰åˆ—ï¼ˆ__ts__ï¼‰ï¼Œ[3]æ˜¯ç¬¬å››åˆ—ï¼ˆ__value__ï¼‰
    ts_str = data_list[0][2]  # æ ¼å¼ï¼š"[1758037443000000000, 1758037503000000000, ...]"
    cpu_str = data_list[0][3]  # æ ¼å¼ï¼š"[0.00231595..., 0.01094574..., ...]"

    # 3. å°†å­—ç¬¦ä¸²åˆ—è¡¨è½¬ä¸ºPythonåˆ—è¡¨ï¼ˆç”¨ast.literal_evalï¼Œé¿å…evalçš„å®‰å…¨é£é™©ï¼‰
    ts_list = ast.literal_eval(ts_str)  # æ—¶é—´æˆ³åˆ—è¡¨ï¼ˆå•ä½ï¼šçº³ç§’ï¼‰
    cpu_list = ast.literal_eval(cpu_str)  # CPUæ•°å€¼åˆ—è¡¨

    # 4. å¤„ç†æ—¶é—´æˆ³ï¼šçº³ç§’è½¬ç§’ï¼ˆé™¤ä»¥1e9ï¼‰ï¼Œå†è½¬ä¸ºdatetimeæ ¼å¼
    timestamps = []
    for ts in ts_list:
        # çº³ç§’ â†’ ç§’ï¼ˆé™¤ä»¥10^9ï¼‰ï¼Œå†è½¬ä¸ºdatetime
        dt = datetime.fromtimestamp(ts / 1e9)
        timestamps.append(dt)

    # 5. ç¡®ä¿CPUå€¼ä¸ºfloatç±»å‹ï¼ˆé˜²æ­¢åŸå§‹æ•°æ®æ˜¯å­—ç¬¦ä¸²ï¼‰
    cpu_values = [float(val) for val in cpu_list]

    return timestamps, cpu_values


def analyze_cpu(normal_start, normal_end, Target_service, show):
    # 1. è®¡ç®—ä¸‰ä¸ªæ—¶æ®µçš„æ—¶é—´æˆ³ï¼ˆè½¬ä¸ºintç±»å‹ï¼ŒCMSæŸ¥è¯¢è¦æ±‚ï¼‰
    # å‰10åˆ†é’Ÿï¼šnormal_start - 10min åˆ° normal_start
    pre10_start = int((normal_start - timedelta(minutes=10)).timestamp())
    pre10_end = int(normal_start.timestamp())
    # æ­£å¸¸æ—¶æ®µï¼šnormal_start åˆ° normal_end
    normal_start_ts = int(normal_start.timestamp())
    normal_end_ts = int(normal_end.timestamp())
    # å10åˆ†é’Ÿï¼šnormal_end åˆ° normal_end + 10min
    post10_start = int(normal_end.timestamp())
    post10_end = int((normal_end + timedelta(minutes=10)).timestamp())

    # 2. CMSæŸ¥è¯¢è¯­å¥ï¼ˆæŸ¥è¯¢deploymentçš„CPUæ€»ä½¿ç”¨ç‡ï¼‰
    query_template = f"""
        .entity_set with(domain='k8s', name='k8s.deployment', query=`deployment='{Target_service}'`)
        | entity-call get_metric('k8s', 'k8s.metric.high_level_metric_deployment', 'deployment_cpu_usage_vs_requests', 'range', '1m')
        """

    # 3. åˆ†åˆ«æŸ¥è¯¢ä¸‰ä¸ªæ—¶æ®µçš„æ•°æ®
    result = cms_tester._execute_spl_query(
        query_template.strip(),
        from_time=pre10_start,
        to_time=post10_end
    )

    timestamps, cpu = get_result(result)

    # 4. åˆ†å‰²ä¸‰ä¸ªæ—¶æ®µçš„æ•°æ®
    pre10_end_dt = datetime.fromtimestamp(pre10_end)
    normal_end_dt = datetime.fromtimestamp(normal_end_ts)
    pre_values, normal_values, post_values = split_time_period_data(
        timestamps, cpu, pre10_end_dt, normal_end_dt
    )

    # 5. å¼‚å¸¸æ£€æµ‹
    is_anomaly, normal_avg, pre_avg, post_avg = detect_anomaly(
        normal_values, pre_values, post_values
    )
    max_cpu = max(normal_values)

    # 6. è¾“å‡ºå¼‚å¸¸æ£€æµ‹ç»“æœ
    print(f"\ncpuå¼‚å¸¸æ£€æµ‹ç»“æœ:")
    print(f"å‰10åˆ†é’Ÿå¹³å‡å€¼: {pre_avg:.4f}")
    print(f"æ£€æµ‹æ—¶æ®µå¹³å‡å€¼: {normal_avg:.4f}")
    print(f"å10åˆ†é’Ÿå¹³å‡å€¼: {post_avg:.4f}")
    print(f"æœ€å¤§CPUä½¿ç”¨ç‡: {max_cpu:.4f}")

    if is_anomaly:
        print(f"ğŸ”´ å¼‚å¸¸æ£€æµ‹: æ£€æµ‹æ—¶æ®µcpuæ˜æ˜¾é«˜äºå‰åæ—¶æ®µ!")
    else:
        print(f"ğŸŸ¢ å¼‚å¸¸æ£€æµ‹: æ£€æµ‹æ—¶æ®µcpuå¤„äºæ­£å¸¸èŒƒå›´")

    if show:
        plt.figure(figsize=(12, 6))

        plt.plot(timestamps, cpu, marker='o', linestyle='-', color='b')
        pre10_start_dt = datetime.fromtimestamp(pre10_start)  # å‰10åˆ†é’Ÿå¼€å§‹ï¼ˆdatetimeï¼‰
        pre10_end_dt = datetime.fromtimestamp(pre10_end)  # å‰10åˆ†é’Ÿç»“æŸï¼ˆdatetimeï¼‰
        normal_start_dt = datetime.fromtimestamp(normal_start_ts)  # ç›®æ ‡æ—¶æ®µå¼€å§‹ï¼ˆdatetimeï¼‰
        normal_end_dt = datetime.fromtimestamp(normal_end_ts)  # ç›®æ ‡æ—¶æ®µç»“æŸï¼ˆdatetimeï¼‰
        post10_start_dt = datetime.fromtimestamp(post10_start)  # å10åˆ†é’Ÿå¼€å§‹ï¼ˆdatetimeï¼‰
        post10_end_dt = datetime.fromtimestamp(post10_end)  # å10åˆ†é’Ÿç»“æŸï¼ˆdatetimeï¼‰
        # ç”¨é˜´å½±æ ‡è®°ä¸‰ä¸ªæ—¶æ®µ
        plt.axvspan(pre10_start_dt, pre10_end_dt, color='lightgreen', alpha=0.3, label='å‰10åˆ†é’Ÿ')
        plt.axvspan(normal_start_dt, normal_end_dt, color='lightcoral', alpha=0.3, label='ç›®æ ‡æ—¶æ®µ')
        plt.axvspan(post10_start_dt, post10_end_dt, color='lightblue', alpha=0.3, label='å10åˆ†é’Ÿ')

        plt.show()

    return is_anomaly, max_cpu, cpu


def analyze_memory(normal_start, normal_end, Target_service, show):
    # 1. è®¡ç®—ä¸‰ä¸ªæ—¶æ®µçš„æ—¶é—´æˆ³ï¼ˆè½¬ä¸ºintç±»å‹ï¼ŒCMSæŸ¥è¯¢è¦æ±‚ï¼‰
    # å‰10åˆ†é’Ÿï¼šnormal_start - 10min åˆ° normal_start
    pre10_start = int((normal_start - timedelta(minutes=10)).timestamp())
    pre10_end = int(normal_start.timestamp())
    # æ­£å¸¸æ—¶æ®µï¼šnormal_start åˆ° normal_end
    normal_start_ts = int(normal_start.timestamp())
    normal_end_ts = int(normal_end.timestamp())
    # å10åˆ†é’Ÿï¼šnormal_end åˆ° normal_end + 10min
    post10_start = int(normal_end.timestamp())
    post10_end = int((normal_end + timedelta(minutes=10)).timestamp())

    # 2. CMSæŸ¥è¯¢è¯­å¥ï¼ˆæŸ¥è¯¢deploymentçš„CPUæ€»ä½¿ç”¨ç‡ï¼‰
    query_template = f"""
        .entity_set with(domain='k8s', name='k8s.deployment', query=`deployment='{Target_service}'`)
        | entity-call get_metric('k8s', 'k8s.metric.high_level_metric_deployment', 'deployment_memory_usage_vs_limits', 'range', '1m')
        """

    # 3. åˆ†åˆ«æŸ¥è¯¢ä¸‰ä¸ªæ—¶æ®µçš„æ•°æ®
    result = cms_tester._execute_spl_query(
        query_template.strip(),
        from_time=pre10_start,
        to_time=post10_end
    )

    timestamps, memory = get_result(result)
    print(timestamps)

    # 4. åˆ†å‰²ä¸‰ä¸ªæ—¶æ®µçš„æ•°æ®
    pre10_end_dt = datetime.fromtimestamp(pre10_end)
    normal_end_dt = datetime.fromtimestamp(normal_end_ts)
    pre_values, normal_values, post_values = split_time_period_data(
        timestamps, memory, pre10_end_dt, normal_end_dt
    )

    # 5. å¼‚å¸¸æ£€æµ‹
    is_anomaly, normal_avg, pre_avg, post_avg = detect_anomaly(
        normal_values, pre_values, post_values
    )
    max_memory = max(normal_values)

    # 6. è¾“å‡ºå¼‚å¸¸æ£€æµ‹ç»“æœ
    print(f"\nmemoryå¼‚å¸¸æ£€æµ‹ç»“æœ:")
    print(f"å‰10åˆ†é’Ÿå¹³å‡å€¼: {pre_avg:.4f}")
    print(f"æ£€æµ‹æ—¶æ®µå¹³å‡å€¼: {normal_avg:.4f}")
    print(f"å10åˆ†é’Ÿå¹³å‡å€¼: {post_avg:.4f}")
    print(f"æœ€å¤§memoryä½¿ç”¨ç‡: {max_memory:.4f}")

    if is_anomaly:
        print(f"ğŸ”´ å¼‚å¸¸æ£€æµ‹: æ£€æµ‹æ—¶æ®µmemoryæ˜æ˜¾é«˜äºå‰åæ—¶æ®µ!")
    else:
        print(f"ğŸŸ¢ å¼‚å¸¸æ£€æµ‹: æ£€æµ‹æ—¶æ®µmemoryå¤„äºæ­£å¸¸èŒƒå›´")

    if show:
        plt.figure(figsize=(12, 6))

        plt.plot(timestamps, memory, marker='o', linestyle='-', color='b')
        pre10_start_dt = datetime.fromtimestamp(pre10_start)  # å‰10åˆ†é’Ÿå¼€å§‹ï¼ˆdatetimeï¼‰
        pre10_end_dt = datetime.fromtimestamp(pre10_end)  # å‰10åˆ†é’Ÿç»“æŸï¼ˆdatetimeï¼‰
        normal_start_dt = datetime.fromtimestamp(normal_start_ts)  # ç›®æ ‡æ—¶æ®µå¼€å§‹ï¼ˆdatetimeï¼‰
        normal_end_dt = datetime.fromtimestamp(normal_end_ts)  # ç›®æ ‡æ—¶æ®µç»“æŸï¼ˆdatetimeï¼‰
        post10_start_dt = datetime.fromtimestamp(post10_start)  # å10åˆ†é’Ÿå¼€å§‹ï¼ˆdatetimeï¼‰
        post10_end_dt = datetime.fromtimestamp(post10_end)  # å10åˆ†é’Ÿç»“æŸï¼ˆdatetimeï¼‰
        # ç”¨é˜´å½±æ ‡è®°ä¸‰ä¸ªæ—¶æ®µ
        plt.axvspan(pre10_start_dt, pre10_end_dt, color='lightgreen', alpha=0.3, label='å‰10åˆ†é’Ÿ')
        plt.axvspan(normal_start_dt, normal_end_dt, color='lightcoral', alpha=0.3, label='ç›®æ ‡æ—¶æ®µ')
        plt.axvspan(post10_start_dt, post10_end_dt, color='lightblue', alpha=0.3, label='å10åˆ†é’Ÿ')
        plt.show()

    return is_anomaly, max_memory, memory

def get_pod_metrics(normal_start, normal_end, Target_pod, show):
    # 1. è®¡ç®—ä¸‰ä¸ªæ—¶æ®µçš„æ—¶é—´æˆ³ï¼ˆè½¬ä¸ºintç±»å‹ï¼ŒCMSæŸ¥è¯¢è¦æ±‚ï¼‰
    # å‰10åˆ†é’Ÿï¼šnormal_start - 10min åˆ° normal_start
    pre10_start = int((normal_start - timedelta(minutes=10)).timestamp())
    pre10_end = int(normal_start.timestamp())
    # æ­£å¸¸æ—¶æ®µï¼šnormal_start åˆ° normal_end
    normal_start_ts = int(normal_start.timestamp())
    normal_end_ts = int(normal_end.timestamp())
    # å10åˆ†é’Ÿï¼šnormal_end åˆ° normal_end + 10min
    post10_start = int(normal_end.timestamp())
    post10_end = int((normal_end + timedelta(minutes=10)).timestamp())

    # 2. CMSæŸ¥è¯¢è¯­å¥ï¼ˆæŸ¥è¯¢deploymentçš„CPUæ€»ä½¿ç”¨ç‡ï¼‰
    query_cpu_template = f"""
            .entity_set with(domain='k8s', name='k8s.pod', query=`name='{Target_pod}'`)
            | entity-call get_golden_metrics('range', '1m')
            """

    # k8s.event.events
    # 3. åˆ†åˆ«æŸ¥è¯¢ä¸‰ä¸ªæ—¶æ®µçš„æ•°æ®
    result = cms_tester._execute_spl_query(
        query_cpu_template.strip(),
        from_time=pre10_start,
        to_time=post10_end
    )
    print(result)
    data_list = result.data
    if not data_list:
        print(f"âš ï¸ ç»“æœä¸­ 'data' å­—æ®µä¸ºç©º")
        return True, []

    timestamps = data_list[0][0]
    timestamps = eval(timestamps)
    print(timestamps)
    timestamps = [
        datetime.fromtimestamp(float(ts) / 1e9)  # çº³ç§’è½¬ç§’ï¼ˆé™¤ä»¥10^9ï¼‰
        for ts in timestamps
    ]
    print("timestamps:", timestamps)
    # åŸå§‹é€»è¾‘ï¼šè·å–cpuæ•°æ®
    cpu = data_list[0][2]
    cpu = eval(cpu)
    # è¿™é‡Œæ ¹æ®éœ€è¦è¿”å›å®é™…æ•°æ®ï¼ˆç¤ºä¾‹ï¼‰
    # 4. åˆ†å‰²ä¸‰ä¸ªæ—¶æ®µçš„æ•°æ®
    pre10_end_dt = datetime.fromtimestamp(pre10_end)
    normal_end_dt = datetime.fromtimestamp(normal_end_ts)
    pre_values, normal_values, post_values = split_time_period_data(
        timestamps, cpu, pre10_end_dt, normal_end_dt
    )

    # 5. å¼‚å¸¸æ£€æµ‹
    is_anomaly, normal_avg, pre_avg, post_avg = detect_anomaly(
        normal_values, pre_values, post_values
    )
    max_cpu = max(normal_values)

    # 6. è¾“å‡ºå¼‚å¸¸æ£€æµ‹ç»“æœ
    print(f"\ncpuå¼‚å¸¸æ£€æµ‹ç»“æœ:")
    print(f"å‰10åˆ†é’Ÿå¹³å‡å€¼: {pre_avg:.4f}")
    print(f"æ£€æµ‹æ—¶æ®µå¹³å‡å€¼: {normal_avg:.4f}")
    print(f"å10åˆ†é’Ÿå¹³å‡å€¼: {post_avg:.4f}")
    print(f"æœ€å¤§cpuä½¿ç”¨ç‡: {max_cpu:.4f}")

    if is_anomaly:
        print(f"ğŸ”´ å¼‚å¸¸æ£€æµ‹: æ£€æµ‹æ—¶æ®µcpuæ˜æ˜¾é«˜äºå‰åæ—¶æ®µ!")
    else:
        print(f"ğŸŸ¢ å¼‚å¸¸æ£€æµ‹: æ£€æµ‹æ—¶æ®µcpuå¤„äºæ­£å¸¸èŒƒå›´")

    if show:
        plt.figure(figsize=(12, 6))

        plt.plot(timestamps, cpu, marker='o', linestyle='-', color='b')
        pre10_start_dt = datetime.fromtimestamp(pre10_start)  # å‰10åˆ†é’Ÿå¼€å§‹ï¼ˆdatetimeï¼‰
        pre10_end_dt = datetime.fromtimestamp(pre10_end)  # å‰10åˆ†é’Ÿç»“æŸï¼ˆdatetimeï¼‰
        normal_start_dt = datetime.fromtimestamp(normal_start_ts)  # ç›®æ ‡æ—¶æ®µå¼€å§‹ï¼ˆdatetimeï¼‰
        normal_end_dt = datetime.fromtimestamp(normal_end_ts)  # ç›®æ ‡æ—¶æ®µç»“æŸï¼ˆdatetimeï¼‰
        post10_start_dt = datetime.fromtimestamp(post10_start)  # å10åˆ†é’Ÿå¼€å§‹ï¼ˆdatetimeï¼‰
        post10_end_dt = datetime.fromtimestamp(post10_end)  # å10åˆ†é’Ÿç»“æŸï¼ˆdatetimeï¼‰
        # ç”¨é˜´å½±æ ‡è®°ä¸‰ä¸ªæ—¶æ®µ
        plt.axvspan(pre10_start_dt, pre10_end_dt, color='lightgreen', alpha=0.3, label='å‰10åˆ†é’Ÿ')
        plt.axvspan(normal_start_dt, normal_end_dt, color='lightcoral', alpha=0.3, label='ç›®æ ‡æ—¶æ®µ')
        plt.axvspan(post10_start_dt, post10_end_dt, color='lightblue', alpha=0.3, label='å10åˆ†é’Ÿ')
        plt.show()

    return cpu

def get_pod(normal_start, normal_end, Target_pod, show):
    # 1. è®¡ç®—ä¸‰ä¸ªæ—¶æ®µçš„æ—¶é—´æˆ³ï¼ˆè½¬ä¸ºintç±»å‹ï¼ŒCMSæŸ¥è¯¢è¦æ±‚ï¼‰
    # å‰10åˆ†é’Ÿï¼šnormal_start - 10min åˆ° normal_start
    pre10_start = int((normal_start - timedelta(minutes=10)).timestamp())
    pre10_end = int(normal_start.timestamp())
    # æ­£å¸¸æ—¶æ®µï¼šnormal_start åˆ° normal_end
    normal_start_ts = int(normal_start.timestamp())
    normal_end_ts = int(normal_end.timestamp())
    # å10åˆ†é’Ÿï¼šnormal_end åˆ° normal_end + 10min
    post10_start = int(normal_end.timestamp())
    post10_end = int((normal_end + timedelta(minutes=10)).timestamp())

    # 2. CMSæŸ¥è¯¢è¯­å¥ï¼ˆæŸ¥è¯¢deploymentçš„CPUæ€»ä½¿ç”¨ç‡ï¼‰
    query_cpu_template = f"""
            .entity_set with(domain='k8s', name='k8s.pod', query=`name='{Target_pod}'`)
            | entity-call get_golden_metrics('range', '1m')
            """

    # k8s.event.events
    # 3. æŸ¥è¯¢æ•°æ®
    result = cms_tester._execute_spl_query(
        query_cpu_template.strip(),
        from_time=pre10_start,
        to_time=post10_end
    )
    print(result)
    data_list = result.data
    if not data_list:
        print(f"âš ï¸ ç»“æœä¸­ 'data' å­—æ®µä¸ºç©º")
        return True, []

    ts_str = data_list[0][0]

    # 4. è®¡ç®—æŸ¥è¯¢æ—¶é—´æ®µçš„æ€»é•¿åº¦ï¼ˆåˆ†é’Ÿï¼‰
    total_duration_minutes = (post10_end - pre10_start) // 60  # æ€»ç§’æ•°è½¬åˆ†é’Ÿ
    expected_points = total_duration_minutes  # 1åˆ†é’Ÿé—´éš”ï¼Œç†è®ºæ•°æ®ç‚¹æ•°é‡

    # 5. è·å–å®é™…æ—¶é—´æˆ³æ•°æ®é•¿åº¦ï¼ˆå‡è®¾ts_stræ˜¯æ—¶é—´æˆ³åˆ—è¡¨çš„å­—ç¬¦ä¸²è¡¨ç¤ºï¼Œéœ€è§£æï¼‰
    # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦æ ¹æ®å®é™…ts_strçš„æ ¼å¼è°ƒæ•´è§£ææ–¹å¼
    # ç¤ºä¾‹ï¼šå¦‚æœts_stræ˜¯"[1620000000, 1620000600, ...]"ï¼Œåˆ™ç”¨evalè½¬æ¢ä¸ºåˆ—è¡¨
    try:
        ts_list = eval(ts_str)  # è§£æå­—ç¬¦ä¸²ä¸ºåˆ—è¡¨
        actual_points = len(ts_list)
    except:
        print(f"âš ï¸ æ— æ³•è§£æts_str: {ts_str}")
        return False, []  # è§£æå¤±è´¥ä¹Ÿè¿”å›False

    # 6. åˆ¤æ–­å®é™…æ•°æ®ç‚¹æ˜¯å¦å°‘äºé¢„æœŸ
    if actual_points < expected_points:
        print(f"âš ï¸ æ•°æ®ç‚¹ä¸å®Œæ•´ï¼šé¢„æœŸ{expected_points}ä¸ªï¼Œå®é™…{actual_points}ä¸ª")
        return False, []

    # åŸå§‹é€»è¾‘ï¼šè·å–cpuæ•°æ®
    cpu_str = data_list[0][2]
    # è¿™é‡Œæ ¹æ®éœ€è¦è¿”å›å®é™…æ•°æ®ï¼ˆç¤ºä¾‹ï¼‰
    return True, [ts_list, cpu_str]


if __name__ == "__main__":
    serveice_list = []
    problem_id = "129"
    input_data = read_input_data("../Bæ¦œé¢˜ç›®.jsonl")
    for problem_data in input_data:
        if problem_data.get("problem_id") == problem_id:
        # if True:
            print(f"ğŸ” Found problem {problem_id}, processing...")
            problem_id = problem_data.get("problem_id", "unknown")
            time_range = problem_data.get("time_range", "")
            candidate_root_causes = problem_data.get("candidate_root_causes", [])
            alarm_rules = problem_data.get("alarm_rules", [])

            start_str, end_str = time_range.split(' ~ ')
            normal_start = datetime.strptime(start_str.strip(), "%Y-%m-%d %H:%M:%S")
            normal_end = datetime.strptime(end_str.strip(), "%Y-%m-%d %H:%M:%S")
            print(f"â° æ­£å¸¸æ—¶æ®µ: {normal_start} ~ {normal_end}")

            target_service = "payment"
            show = True
            cpu_anomaly = analyze_cpu(normal_start, normal_end, target_service, show)
            memory_anomaly = analyze_memory(normal_start, normal_end, target_service, show)
            print(f"CPUå¼‚å¸¸: {cpu_anomaly}, Memoryå¼‚å¸¸: {memory_anomaly}")
            if cpu_anomaly[0]:
                print(f"å­˜åœ¨å¼‚å¸¸")
            else:
                print(f"âœ… æ­£å¸¸")
            break
